{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6feab7ff-456e-494d-878f-522e0dc3149d",
   "metadata": {},
   "source": [
    "# Numpy Neural Network\n",
    "\n",
    "Inspired by code [here](https://github.com/MichalDanielDobrzanski/DeepLearningPython)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcead36-c452-4634-a147-6fa348e0ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0be303-9f76-45f7-9149-89cd12d792c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e630a-e470-40fc-bcb3-cc9e5651263e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5dc426a-a738-4ad7-b613-f4485e09526c",
   "metadata": {},
   "source": [
    "## network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc80851-4184-494c-851a-0d2f247e967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                print(f\"Epoch {j} : {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x \n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac78767-2a51-4b11-a275-9c998112a9ae",
   "metadata": {},
   "source": [
    "## mnist_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8847f179-6801-4d04-826e-bc875fc3554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e        \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293d58af-45cb-4665-9896-2414587a5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data = list(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ba9218-7021-48ad-bd19-f1d6d47ad1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9099 / 10000\n",
      "Epoch 1 : 9177 / 10000\n",
      "Epoch 2 : 9314 / 10000\n",
      "Epoch 3 : 9374 / 10000\n",
      "Epoch 4 : 9348 / 10000\n",
      "Epoch 5 : 9424 / 10000\n",
      "Epoch 6 : 9436 / 10000\n",
      "Epoch 7 : 9453 / 10000\n",
      "Epoch 8 : 9455 / 10000\n",
      "Epoch 9 : 9485 / 10000\n",
      "Epoch 10 : 9508 / 10000\n",
      "Epoch 11 : 9476 / 10000\n",
      "Epoch 12 : 9486 / 10000\n",
      "Epoch 13 : 9523 / 10000\n",
      "Epoch 14 : 9509 / 10000\n",
      "Epoch 15 : 9500 / 10000\n",
      "Epoch 16 : 9511 / 10000\n",
      "Epoch 17 : 9503 / 10000\n",
      "Epoch 18 : 9492 / 10000\n",
      "Epoch 19 : 9513 / 10000\n",
      "Epoch 20 : 9510 / 10000\n",
      "Epoch 21 : 9534 / 10000\n",
      "Epoch 22 : 9534 / 10000\n",
      "Epoch 23 : 9532 / 10000\n",
      "Epoch 24 : 9528 / 10000\n",
      "Epoch 25 : 9549 / 10000\n",
      "Epoch 26 : 9520 / 10000\n",
      "Epoch 27 : 9510 / 10000\n",
      "Epoch 28 : 9539 / 10000\n",
      "Epoch 29 : 9531 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eaecb3-22d2-4961-9afb-faae439984ad",
   "metadata": {},
   "source": [
    "## network2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bef7570-1f5c-4b46-9c1f-5bf21289c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost:\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "    \n",
    "    \n",
    "class CrossEntropyCost:\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "    \n",
    "class Network:\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]    \n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, \n",
    "           lmbda = 0.0,\n",
    "           evaluation_data=None,\n",
    "           monitor_evaluation_cost=False,\n",
    "           monitor_evaluation_accuracy=False,\n",
    "           monitor_training_cost=False,\n",
    "           monitor_training_accuracy=False,\n",
    "           early_stopping_n=0):\n",
    "        best_accuracy=1\n",
    "        \n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if evaluation_data:\n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "            \n",
    "            \n",
    "        best_accuracy=0\n",
    "        no_accuracy_change=0\n",
    "        \n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "                \n",
    "            print(f\"Epoch {j} training complete\")\n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(f\"Cost on training data: {cost}\")\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(f\"Accuracy on training data: {accuracy} / {n}\")\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(f\"Cost on evaluation data: {cost}\")\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(f\"Accuracy on evaluation data: {accuracy} / {n_data}\")\n",
    "                \n",
    "            # Early stopping\n",
    "            if early_stopping_n > 0:\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    no_accuracy_change = 0\n",
    "                else:\n",
    "                    no_accuracy_change +=1\n",
    "                    \n",
    "                if no_accuracy_change == early_stopping_n:\n",
    "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "                \n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb +dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x \n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def accuracy(self, data, convert=False):\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy\n",
    "    \n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def save(self, filename):\n",
    "        data = {\n",
    "            \"sizes\":self.sizes,\n",
    "            \"weights\":[w.tolist() for w in self.weights],\n",
    "            \"biases\": [b.tolist() for b in self.biases],\n",
    "            \"cost\": str(self.cost.__name__)\n",
    "        }\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "        \n",
    "    def load(filename):\n",
    "        f = open(filename, \"r\")\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "        cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "        net = Network(data[\"sizes\"], cost=cost)\n",
    "        net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "        net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd5afe-f4b9-400a-bdc8-e317e452b3fb",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83498e6-68f0-4852-a9e6-a8be09942dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9237 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9417 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9465 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9508 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9503 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9507 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9569 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9579 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9573 / 10000\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9588 / 10000\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9614 / 10000\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9612 / 10000\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9598 / 10000\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9620 / 10000\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9638 / 10000\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9625 / 10000\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9624 / 10000\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9621 / 10000\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9594 / 10000\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9628 / 10000\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9615 / 10000\n",
      "Epoch 30 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 31 training complete\n",
      "Accuracy on evaluation data: 9634 / 10000\n",
      "Epoch 32 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 33 training complete\n",
      "Accuracy on evaluation data: 9625 / 10000\n",
      "Epoch 34 training complete\n",
      "Accuracy on evaluation data: 9653 / 10000\n",
      "Epoch 35 training complete\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "Epoch 36 training complete\n",
      "Accuracy on evaluation data: 9621 / 10000\n",
      "Epoch 37 training complete\n",
      "Accuracy on evaluation data: 9622 / 10000\n",
      "Epoch 38 training complete\n",
      "Accuracy on evaluation data: 9627 / 10000\n",
      "Epoch 39 training complete\n",
      "Accuracy on evaluation data: 9627 / 10000\n",
      "Epoch 40 training complete\n",
      "Accuracy on evaluation data: 9650 / 10000\n",
      "Epoch 41 training complete\n",
      "Accuracy on evaluation data: 9658 / 10000\n",
      "Epoch 42 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 43 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "Epoch 44 training complete\n",
      "Accuracy on evaluation data: 9644 / 10000\n",
      "Epoch 45 training complete\n",
      "Accuracy on evaluation data: 9623 / 10000\n",
      "Epoch 46 training complete\n",
      "Accuracy on evaluation data: 9642 / 10000\n",
      "Epoch 47 training complete\n",
      "Accuracy on evaluation data: 9643 / 10000\n",
      "Epoch 48 training complete\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "Epoch 49 training complete\n",
      "Accuracy on evaluation data: 9647 / 10000\n",
      "Epoch 50 training complete\n",
      "Accuracy on evaluation data: 9653 / 10000\n",
      "Epoch 51 training complete\n",
      "Accuracy on evaluation data: 9635 / 10000\n",
      "Epoch 52 training complete\n",
      "Accuracy on evaluation data: 9644 / 10000\n",
      "Epoch 53 training complete\n",
      "Accuracy on evaluation data: 9624 / 10000\n",
      "Epoch 54 training complete\n",
      "Accuracy on evaluation data: 9607 / 10000\n",
      "Epoch 55 training complete\n",
      "Accuracy on evaluation data: 9627 / 10000\n",
      "Epoch 56 training complete\n",
      "Accuracy on evaluation data: 9638 / 10000\n",
      "Epoch 57 training complete\n",
      "Accuracy on evaluation data: 9654 / 10000\n",
      "Epoch 58 training complete\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "Epoch 59 training complete\n",
      "Accuracy on evaluation data: 9648 / 10000\n",
      "Epoch 60 training complete\n",
      "Accuracy on evaluation data: 9646 / 10000\n",
      "Epoch 61 training complete\n",
      "Accuracy on evaluation data: 9648 / 10000\n",
      "Epoch 62 training complete\n",
      "Accuracy on evaluation data: 9663 / 10000\n",
      "Epoch 63 training complete\n",
      "Accuracy on evaluation data: 9647 / 10000\n",
      "Epoch 64 training complete\n",
      "Accuracy on evaluation data: 9628 / 10000\n",
      "Epoch 65 training complete\n",
      "Accuracy on evaluation data: 9657 / 10000\n",
      "Epoch 66 training complete\n",
      "Accuracy on evaluation data: 9650 / 10000\n",
      "Epoch 67 training complete\n",
      "Accuracy on evaluation data: 9647 / 10000\n",
      "Epoch 68 training complete\n",
      "Accuracy on evaluation data: 9653 / 10000\n",
      "Epoch 69 training complete\n",
      "Accuracy on evaluation data: 9653 / 10000\n",
      "Epoch 70 training complete\n",
      "Accuracy on evaluation data: 9637 / 10000\n",
      "Epoch 71 training complete\n",
      "Accuracy on evaluation data: 9645 / 10000\n",
      "Epoch 72 training complete\n",
      "Accuracy on evaluation data: 9635 / 10000\n",
      "Epoch 73 training complete\n",
      "Accuracy on evaluation data: 9635 / 10000\n",
      "Epoch 74 training complete\n",
      "Accuracy on evaluation data: 9651 / 10000\n",
      "Epoch 75 training complete\n",
      "Accuracy on evaluation data: 9655 / 10000\n",
      "Epoch 76 training complete\n",
      "Accuracy on evaluation data: 9656 / 10000\n",
      "Epoch 77 training complete\n",
      "Accuracy on evaluation data: 9665 / 10000\n",
      "Epoch 78 training complete\n",
      "Accuracy on evaluation data: 9657 / 10000\n",
      "Epoch 79 training complete\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "Epoch 80 training complete\n",
      "Accuracy on evaluation data: 9655 / 10000\n",
      "Epoch 81 training complete\n",
      "Accuracy on evaluation data: 9659 / 10000\n",
      "Epoch 82 training complete\n",
      "Accuracy on evaluation data: 9650 / 10000\n",
      "Epoch 83 training complete\n",
      "Accuracy on evaluation data: 9660 / 10000\n",
      "Epoch 84 training complete\n",
      "Accuracy on evaluation data: 9627 / 10000\n",
      "Epoch 85 training complete\n",
      "Accuracy on evaluation data: 9646 / 10000\n",
      "Epoch 86 training complete\n",
      "Accuracy on evaluation data: 9650 / 10000\n",
      "Epoch 87 training complete\n",
      "Accuracy on evaluation data: 9664 / 10000\n",
      "Epoch 88 training complete\n",
      "Accuracy on evaluation data: 9653 / 10000\n",
      "Epoch 89 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "Epoch 90 training complete\n",
      "Accuracy on evaluation data: 9657 / 10000\n",
      "Epoch 91 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "Epoch 92 training complete\n",
      "Accuracy on evaluation data: 9634 / 10000\n",
      "Epoch 93 training complete\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "Epoch 94 training complete\n",
      "Accuracy on evaluation data: 9654 / 10000\n",
      "Epoch 95 training complete\n",
      "Accuracy on evaluation data: 9640 / 10000\n",
      "Epoch 96 training complete\n",
      "Accuracy on evaluation data: 9658 / 10000\n",
      "Epoch 97 training complete\n",
      "Accuracy on evaluation data: 9658 / 10000\n",
      "Epoch 98 training complete\n",
      "Accuracy on evaluation data: 9655 / 10000\n",
      "Epoch 99 training complete\n",
      "Accuracy on evaluation data: 9650 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9237,\n",
       "  9417,\n",
       "  9465,\n",
       "  9508,\n",
       "  9503,\n",
       "  9507,\n",
       "  9569,\n",
       "  9579,\n",
       "  9562,\n",
       "  9573,\n",
       "  9588,\n",
       "  9574,\n",
       "  9595,\n",
       "  9610,\n",
       "  9614,\n",
       "  9612,\n",
       "  9598,\n",
       "  9607,\n",
       "  9620,\n",
       "  9607,\n",
       "  9629,\n",
       "  9638,\n",
       "  9625,\n",
       "  9624,\n",
       "  9621,\n",
       "  9632,\n",
       "  9594,\n",
       "  9628,\n",
       "  9607,\n",
       "  9615,\n",
       "  9632,\n",
       "  9634,\n",
       "  9632,\n",
       "  9625,\n",
       "  9653,\n",
       "  9629,\n",
       "  9621,\n",
       "  9622,\n",
       "  9627,\n",
       "  9627,\n",
       "  9650,\n",
       "  9658,\n",
       "  9632,\n",
       "  9652,\n",
       "  9644,\n",
       "  9623,\n",
       "  9642,\n",
       "  9643,\n",
       "  9641,\n",
       "  9647,\n",
       "  9653,\n",
       "  9635,\n",
       "  9644,\n",
       "  9624,\n",
       "  9607,\n",
       "  9627,\n",
       "  9638,\n",
       "  9654,\n",
       "  9632,\n",
       "  9648,\n",
       "  9646,\n",
       "  9648,\n",
       "  9663,\n",
       "  9647,\n",
       "  9628,\n",
       "  9657,\n",
       "  9650,\n",
       "  9647,\n",
       "  9653,\n",
       "  9653,\n",
       "  9637,\n",
       "  9645,\n",
       "  9635,\n",
       "  9635,\n",
       "  9651,\n",
       "  9655,\n",
       "  9656,\n",
       "  9665,\n",
       "  9657,\n",
       "  9629,\n",
       "  9655,\n",
       "  9659,\n",
       "  9650,\n",
       "  9660,\n",
       "  9627,\n",
       "  9646,\n",
       "  9650,\n",
       "  9664,\n",
       "  9653,\n",
       "  9652,\n",
       "  9657,\n",
       "  9652,\n",
       "  9634,\n",
       "  9641,\n",
       "  9654,\n",
       "  9640,\n",
       "  9658,\n",
       "  9658,\n",
       "  9655,\n",
       "  9650],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "# net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2cc79-84ea-4e31-b2dd-392d1cbb1c00",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd030dfa-a424-4a32-8557-02dd61b36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 2.318560164532979\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.5914227178620113\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.2534895290552646\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.0253958025712793\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.8671122704172909\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.8080035917493397\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.7041638021491149\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.6100652205318042\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.5730831263121682\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.49952649874465677\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.446350384577229\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.42610675854338775\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.36620958855882724\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.34879128749480354\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.32051953184226983\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.29457634174569836\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.27164955911539906\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.2564143273502131\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.23433229029353317\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.22279916603857788\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.21205344228922104\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.20171566444349615\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.18411846098769002\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.17570236567630945\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.1628022418311573\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.15508222209859993\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.14884146100985793\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.1410107035186565\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.13348173991908227\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.12657409098523992\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.12094538545354303\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.11495446596944667\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.11119348490310756\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.10653385778869817\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.10386337978563091\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.09985622090714896\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.09506772311494195\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.0924943127757385\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.09068878472773666\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.08543973422325331\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.08311645570360109\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.08158084883652635\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.07807165516242258\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.07609223612050917\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.07325610530280513\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.07132187458640289\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.06957443712869699\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.06695747507359982\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.06542108779606184\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.06344986269303364\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.06183803366462837\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.0599987393934963\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.05800645119140003\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.05647204029327193\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.055035055274077746\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.053945829006151115\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.05265433936283529\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.051069505018136195\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.05002400409406816\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.04904648394521267\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.04792390712405135\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.046661702212247934\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.0457042078346054\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.044811236129067844\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.043946484181168935\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.04287804129610555\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.042083726574693024\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.041070243180357596\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.04006408451330312\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.039423694050021434\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.03842602825467458\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.03771933059760569\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.037045028957886625\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.03638568921900631\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.035722020268244496\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.03500113954660121\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.034445838336099634\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.033937518919684884\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.03323780215317198\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.032763579749145566\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.032221377791454876\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.03175334011497108\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.03125409886414791\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.03083862741074332\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.03028661812690346\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.029831822825527644\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.029339528812525665\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.028919855286470883\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.02848426183577125\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.02806561187116546\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.02764258485605491\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.02725009151167478\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.026943941038458145\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.026763788410758736\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.026164602393029937\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.02575743834278583\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.02543848447906492\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.02507642148885758\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.02470073758435718\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.024386549944489144\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.024041252287947942\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.02379156747153951\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.02339955350827637\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.023132388693136205\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.022806530888894283\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.02247345056108409\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.02215901895954103\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.02187485041285869\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.021575184897994815\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.021218473098900893\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.02090776696589088\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.020626425298466127\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.020325225612890897\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.020068474822073988\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.019795655593591335\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.019561893750812482\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.019357940523839996\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.019080933721191202\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.018878500972830885\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.018697652057151397\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.018465862211904095\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.018223208941264193\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.018016007985513446\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.01782557742531481\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.01762903670410374\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.01745143339178072\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.017267947025153302\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.017086835452910662\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.016906737642999905\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.016746593697943464\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.016570927950396427\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.0164394953374783\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.01623794395180755\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.016090840197946106\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.015931912982220322\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.01578631771246586\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.01562697865905212\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.015482655769402779\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.015367309975057938\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.01520122103617426\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.0150642632575039\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.014929459435426408\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.01479714783259843\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.01466378586646506\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.014534260853956047\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.014408604955359216\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.01429452407346515\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.014164377902464845\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.014055117092455684\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.013938132807243136\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.013841595170023376\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.013691867920000317\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.013581850153214227\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.013468716901496865\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.0133641110472922\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.013258966834676384\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.013147228150880498\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.01304350640580356\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.012946033241232802\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.012841936249135965\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.01274298094056839\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.012643531549770203\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.012545213695740791\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.01246447665298051\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.012365056866464249\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.01226515615537771\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.012184906070508091\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.012090838611579035\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.012007818974321089\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.011913317646783352\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.01182013679508593\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.01174368977541958\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.011650730570551849\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.011570790531059976\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.011484067215448784\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.011404605567016246\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.011324486625389492\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.01125823831129507\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.01116684975946533\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.011088604344307341\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.011024722291944726\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.01094675730871702\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.010865467033188681\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.01079422169546711\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.010714650582781527\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.010641753316282627\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.010571611809118477\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.010501940015281274\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.010429765988075872\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.010362114220945738\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.010294360388874718\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.010226433176703246\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.010161279489559047\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.010090872513171808\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.010025113586816526\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.009960667561658436\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.009896809741265762\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.009837872818890135\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.00976977051097116\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.009708711355669522\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.009647307439656002\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.009584727274260123\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.009526727726774759\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.009466932096672334\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.009406586263403401\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.00934909355110945\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.009290501320205516\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.009233054295966556\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.009176459309552836\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.0091212620985767\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.009066586557279044\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.009013270032733956\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.008958270830228583\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.008904488743078792\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.00885190078278952\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.008800247230821337\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.008747962753231545\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.008698367822473043\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.008648248290118881\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.008598260030655844\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.008548784551283741\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.008500228656942926\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.00845200674717051\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.008406740142613662\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.008358583178959182\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.008311788519992692\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.008264736661292519\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.008222173802206287\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.008175315240591669\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.008131038929073223\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.008085628551764383\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.008041835861378684\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.00799851060365536\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.007956554168202455\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.007913700852410582\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.007872362974271619\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.007830684613461305\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.007789529633263935\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.007750444784210014\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.007707229939204794\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.007668443969943912\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.00762797352389676\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.007588338073727393\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.0075503027227724106\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.007511839295296857\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.007474324920053706\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.007434170311272293\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.007396110478103621\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.007358940487561176\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.007321845552713469\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.007285287243333355\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.007248737838935489\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.00721264564375335\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.007176529810432384\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.00714138424232838\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.007105753551924746\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.007071200892610641\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.007036603868522118\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.007003182368429937\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.0069690884877666994\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.00693569896457154\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.0069012648385324265\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.006869271663585388\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.006835650404824373\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.006804017512579153\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.006771220796626454\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.006738905246674747\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.0067071982421276315\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.006676220872409086\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.006644589939635943\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.006614108762293875\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.006583265475840751\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.006553101070533637\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.00652464920283512\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.006495855865214183\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.006464640662975142\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.006435918151714115\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.006406517722133249\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.006378381197268592\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.0063500165798870845\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.006321933250398714\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.0062942714562844344\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.006267561008602671\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.006239605885350893\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.006212419555036818\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.006185682857353911\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.006159264998269411\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.006133715483404755\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.006107417826110074\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.00608144885901494\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.00605653526652755\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.006030862912623522\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.00600634344741774\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.005981466598431936\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.005957208387722836\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.005932598332687984\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.005908925229508552\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.005883954806888699\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.0058599447751058915\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.005836266271953606\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.0058128532357060855\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.00578984810818154\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.005766783622819765\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.005744104113707407\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.005721854207137004\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.005699318068519795\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.005677383765558582\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.005655501966377716\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.005634085115295204\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.005612393625628571\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.005590006514206438\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.00556814185496527\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.005547201872140772\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.005526644520816322\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.005504982288277711\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.005484431624202516\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.0054642157652373645\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.005443457048174971\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.005423614213624305\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.0054042834984049624\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.005383584344064782\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.005363945343834289\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.005344473905385214\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.005324415229746312\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.005305268127186298\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.0052857859524449375\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.005266402288283664\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.005247749974993918\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.005228669818422526\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.005209741184059503\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.005191319237268388\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.0051729649350001\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.00515463954283762\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.005136778603069286\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.005118029390891387\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.005100030680034586\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.00508222022149464\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.005064625120657236\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.005047033279541875\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.005029753672993416\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.005012302608156916\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.004994836072879268\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.004977673652530252\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.004960749502005808\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.004943622091047605\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.004926911828267384\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.004910050696608799\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.0048935494256767825\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.0048775248200043485\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.004860619854298815\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.004844521805852133\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.00482821471359531\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.004812078861721341\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.004796367229374768\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.004780285480204613\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.004764657394221861\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.004748698861828723\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.004733023572844969\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.004717438462403533\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.004701917231559372\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.004686598557015776\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.004671362562971546\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.004656256502160199\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.00464110120291647\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.004626099580905798\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.004611230990994321\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.004596349595393011\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.004581657188642643\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.004566954436437581\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.004552445988727392\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.00453794835743735\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.004523494400378598\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.00450917430708993\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.0044950934773811705\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.004480827288481321\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.0044667992375869384\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.00445267092812081\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.004438799595853825\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.00442484966541765\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.004411057287288909\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.004397290273941155\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.004383705925752901\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.004370056821088258\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.004356707100331503\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.004343207662604065\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.0043297637085786085\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.00431652388686047\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.004303137313178655\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.004289998013986255\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.004276823442209902\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.004263852420710589\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.00425088380978556\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.004237934784270178\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.004225122392374868\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.004212267864105172\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.0041994561444102805\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.0041867561690741035\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.004174261998316169\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.004161599844289446\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.0041490440106191386\n",
      "Accuracy on evaluation data: 0 / 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2.318560164532979,\n",
       "  1.5914227178620113,\n",
       "  1.2534895290552646,\n",
       "  1.0253958025712793,\n",
       "  0.8671122704172909,\n",
       "  0.8080035917493397,\n",
       "  0.7041638021491149,\n",
       "  0.6100652205318042,\n",
       "  0.5730831263121682,\n",
       "  0.49952649874465677,\n",
       "  0.446350384577229,\n",
       "  0.42610675854338775,\n",
       "  0.36620958855882724,\n",
       "  0.34879128749480354,\n",
       "  0.32051953184226983,\n",
       "  0.29457634174569836,\n",
       "  0.27164955911539906,\n",
       "  0.2564143273502131,\n",
       "  0.23433229029353317,\n",
       "  0.22279916603857788,\n",
       "  0.21205344228922104,\n",
       "  0.20171566444349615,\n",
       "  0.18411846098769002,\n",
       "  0.17570236567630945,\n",
       "  0.1628022418311573,\n",
       "  0.15508222209859993,\n",
       "  0.14884146100985793,\n",
       "  0.1410107035186565,\n",
       "  0.13348173991908227,\n",
       "  0.12657409098523992,\n",
       "  0.12094538545354303,\n",
       "  0.11495446596944667,\n",
       "  0.11119348490310756,\n",
       "  0.10653385778869817,\n",
       "  0.10386337978563091,\n",
       "  0.09985622090714896,\n",
       "  0.09506772311494195,\n",
       "  0.0924943127757385,\n",
       "  0.09068878472773666,\n",
       "  0.08543973422325331,\n",
       "  0.08311645570360109,\n",
       "  0.08158084883652635,\n",
       "  0.07807165516242258,\n",
       "  0.07609223612050917,\n",
       "  0.07325610530280513,\n",
       "  0.07132187458640289,\n",
       "  0.06957443712869699,\n",
       "  0.06695747507359982,\n",
       "  0.06542108779606184,\n",
       "  0.06344986269303364,\n",
       "  0.06183803366462837,\n",
       "  0.0599987393934963,\n",
       "  0.05800645119140003,\n",
       "  0.05647204029327193,\n",
       "  0.055035055274077746,\n",
       "  0.053945829006151115,\n",
       "  0.05265433936283529,\n",
       "  0.051069505018136195,\n",
       "  0.05002400409406816,\n",
       "  0.04904648394521267,\n",
       "  0.04792390712405135,\n",
       "  0.046661702212247934,\n",
       "  0.0457042078346054,\n",
       "  0.044811236129067844,\n",
       "  0.043946484181168935,\n",
       "  0.04287804129610555,\n",
       "  0.042083726574693024,\n",
       "  0.041070243180357596,\n",
       "  0.04006408451330312,\n",
       "  0.039423694050021434,\n",
       "  0.03842602825467458,\n",
       "  0.03771933059760569,\n",
       "  0.037045028957886625,\n",
       "  0.03638568921900631,\n",
       "  0.035722020268244496,\n",
       "  0.03500113954660121,\n",
       "  0.034445838336099634,\n",
       "  0.033937518919684884,\n",
       "  0.03323780215317198,\n",
       "  0.032763579749145566,\n",
       "  0.032221377791454876,\n",
       "  0.03175334011497108,\n",
       "  0.03125409886414791,\n",
       "  0.03083862741074332,\n",
       "  0.03028661812690346,\n",
       "  0.029831822825527644,\n",
       "  0.029339528812525665,\n",
       "  0.028919855286470883,\n",
       "  0.02848426183577125,\n",
       "  0.02806561187116546,\n",
       "  0.02764258485605491,\n",
       "  0.02725009151167478,\n",
       "  0.026943941038458145,\n",
       "  0.026763788410758736,\n",
       "  0.026164602393029937,\n",
       "  0.02575743834278583,\n",
       "  0.02543848447906492,\n",
       "  0.02507642148885758,\n",
       "  0.02470073758435718,\n",
       "  0.024386549944489144,\n",
       "  0.024041252287947942,\n",
       "  0.02379156747153951,\n",
       "  0.02339955350827637,\n",
       "  0.023132388693136205,\n",
       "  0.022806530888894283,\n",
       "  0.02247345056108409,\n",
       "  0.02215901895954103,\n",
       "  0.02187485041285869,\n",
       "  0.021575184897994815,\n",
       "  0.021218473098900893,\n",
       "  0.02090776696589088,\n",
       "  0.020626425298466127,\n",
       "  0.020325225612890897,\n",
       "  0.020068474822073988,\n",
       "  0.019795655593591335,\n",
       "  0.019561893750812482,\n",
       "  0.019357940523839996,\n",
       "  0.019080933721191202,\n",
       "  0.018878500972830885,\n",
       "  0.018697652057151397,\n",
       "  0.018465862211904095,\n",
       "  0.018223208941264193,\n",
       "  0.018016007985513446,\n",
       "  0.01782557742531481,\n",
       "  0.01762903670410374,\n",
       "  0.01745143339178072,\n",
       "  0.017267947025153302,\n",
       "  0.017086835452910662,\n",
       "  0.016906737642999905,\n",
       "  0.016746593697943464,\n",
       "  0.016570927950396427,\n",
       "  0.0164394953374783,\n",
       "  0.01623794395180755,\n",
       "  0.016090840197946106,\n",
       "  0.015931912982220322,\n",
       "  0.01578631771246586,\n",
       "  0.01562697865905212,\n",
       "  0.015482655769402779,\n",
       "  0.015367309975057938,\n",
       "  0.01520122103617426,\n",
       "  0.0150642632575039,\n",
       "  0.014929459435426408,\n",
       "  0.01479714783259843,\n",
       "  0.01466378586646506,\n",
       "  0.014534260853956047,\n",
       "  0.014408604955359216,\n",
       "  0.01429452407346515,\n",
       "  0.014164377902464845,\n",
       "  0.014055117092455684,\n",
       "  0.013938132807243136,\n",
       "  0.013841595170023376,\n",
       "  0.013691867920000317,\n",
       "  0.013581850153214227,\n",
       "  0.013468716901496865,\n",
       "  0.0133641110472922,\n",
       "  0.013258966834676384,\n",
       "  0.013147228150880498,\n",
       "  0.01304350640580356,\n",
       "  0.012946033241232802,\n",
       "  0.012841936249135965,\n",
       "  0.01274298094056839,\n",
       "  0.012643531549770203,\n",
       "  0.012545213695740791,\n",
       "  0.01246447665298051,\n",
       "  0.012365056866464249,\n",
       "  0.01226515615537771,\n",
       "  0.012184906070508091,\n",
       "  0.012090838611579035,\n",
       "  0.012007818974321089,\n",
       "  0.011913317646783352,\n",
       "  0.01182013679508593,\n",
       "  0.01174368977541958,\n",
       "  0.011650730570551849,\n",
       "  0.011570790531059976,\n",
       "  0.011484067215448784,\n",
       "  0.011404605567016246,\n",
       "  0.011324486625389492,\n",
       "  0.01125823831129507,\n",
       "  0.01116684975946533,\n",
       "  0.011088604344307341,\n",
       "  0.011024722291944726,\n",
       "  0.01094675730871702,\n",
       "  0.010865467033188681,\n",
       "  0.01079422169546711,\n",
       "  0.010714650582781527,\n",
       "  0.010641753316282627,\n",
       "  0.010571611809118477,\n",
       "  0.010501940015281274,\n",
       "  0.010429765988075872,\n",
       "  0.010362114220945738,\n",
       "  0.010294360388874718,\n",
       "  0.010226433176703246,\n",
       "  0.010161279489559047,\n",
       "  0.010090872513171808,\n",
       "  0.010025113586816526,\n",
       "  0.009960667561658436,\n",
       "  0.009896809741265762,\n",
       "  0.009837872818890135,\n",
       "  0.00976977051097116,\n",
       "  0.009708711355669522,\n",
       "  0.009647307439656002,\n",
       "  0.009584727274260123,\n",
       "  0.009526727726774759,\n",
       "  0.009466932096672334,\n",
       "  0.009406586263403401,\n",
       "  0.00934909355110945,\n",
       "  0.009290501320205516,\n",
       "  0.009233054295966556,\n",
       "  0.009176459309552836,\n",
       "  0.0091212620985767,\n",
       "  0.009066586557279044,\n",
       "  0.009013270032733956,\n",
       "  0.008958270830228583,\n",
       "  0.008904488743078792,\n",
       "  0.00885190078278952,\n",
       "  0.008800247230821337,\n",
       "  0.008747962753231545,\n",
       "  0.008698367822473043,\n",
       "  0.008648248290118881,\n",
       "  0.008598260030655844,\n",
       "  0.008548784551283741,\n",
       "  0.008500228656942926,\n",
       "  0.00845200674717051,\n",
       "  0.008406740142613662,\n",
       "  0.008358583178959182,\n",
       "  0.008311788519992692,\n",
       "  0.008264736661292519,\n",
       "  0.008222173802206287,\n",
       "  0.008175315240591669,\n",
       "  0.008131038929073223,\n",
       "  0.008085628551764383,\n",
       "  0.008041835861378684,\n",
       "  0.00799851060365536,\n",
       "  0.007956554168202455,\n",
       "  0.007913700852410582,\n",
       "  0.007872362974271619,\n",
       "  0.007830684613461305,\n",
       "  0.007789529633263935,\n",
       "  0.007750444784210014,\n",
       "  0.007707229939204794,\n",
       "  0.007668443969943912,\n",
       "  0.00762797352389676,\n",
       "  0.007588338073727393,\n",
       "  0.0075503027227724106,\n",
       "  0.007511839295296857,\n",
       "  0.007474324920053706,\n",
       "  0.007434170311272293,\n",
       "  0.007396110478103621,\n",
       "  0.007358940487561176,\n",
       "  0.007321845552713469,\n",
       "  0.007285287243333355,\n",
       "  0.007248737838935489,\n",
       "  0.00721264564375335,\n",
       "  0.007176529810432384,\n",
       "  0.00714138424232838,\n",
       "  0.007105753551924746,\n",
       "  0.007071200892610641,\n",
       "  0.007036603868522118,\n",
       "  0.007003182368429937,\n",
       "  0.0069690884877666994,\n",
       "  0.00693569896457154,\n",
       "  0.0069012648385324265,\n",
       "  0.006869271663585388,\n",
       "  0.006835650404824373,\n",
       "  0.006804017512579153,\n",
       "  0.006771220796626454,\n",
       "  0.006738905246674747,\n",
       "  0.0067071982421276315,\n",
       "  0.006676220872409086,\n",
       "  0.006644589939635943,\n",
       "  0.006614108762293875,\n",
       "  0.006583265475840751,\n",
       "  0.006553101070533637,\n",
       "  0.00652464920283512,\n",
       "  0.006495855865214183,\n",
       "  0.006464640662975142,\n",
       "  0.006435918151714115,\n",
       "  0.006406517722133249,\n",
       "  0.006378381197268592,\n",
       "  0.0063500165798870845,\n",
       "  0.006321933250398714,\n",
       "  0.0062942714562844344,\n",
       "  0.006267561008602671,\n",
       "  0.006239605885350893,\n",
       "  0.006212419555036818,\n",
       "  0.006185682857353911,\n",
       "  0.006159264998269411,\n",
       "  0.006133715483404755,\n",
       "  0.006107417826110074,\n",
       "  0.00608144885901494,\n",
       "  0.00605653526652755,\n",
       "  0.006030862912623522,\n",
       "  0.00600634344741774,\n",
       "  0.005981466598431936,\n",
       "  0.005957208387722836,\n",
       "  0.005932598332687984,\n",
       "  0.005908925229508552,\n",
       "  0.005883954806888699,\n",
       "  0.0058599447751058915,\n",
       "  0.005836266271953606,\n",
       "  0.0058128532357060855,\n",
       "  0.00578984810818154,\n",
       "  0.005766783622819765,\n",
       "  0.005744104113707407,\n",
       "  0.005721854207137004,\n",
       "  0.005699318068519795,\n",
       "  0.005677383765558582,\n",
       "  0.005655501966377716,\n",
       "  0.005634085115295204,\n",
       "  0.005612393625628571,\n",
       "  0.005590006514206438,\n",
       "  0.00556814185496527,\n",
       "  0.005547201872140772,\n",
       "  0.005526644520816322,\n",
       "  0.005504982288277711,\n",
       "  0.005484431624202516,\n",
       "  0.0054642157652373645,\n",
       "  0.005443457048174971,\n",
       "  0.005423614213624305,\n",
       "  0.0054042834984049624,\n",
       "  0.005383584344064782,\n",
       "  0.005363945343834289,\n",
       "  0.005344473905385214,\n",
       "  0.005324415229746312,\n",
       "  0.005305268127186298,\n",
       "  0.0052857859524449375,\n",
       "  0.005266402288283664,\n",
       "  0.005247749974993918,\n",
       "  0.005228669818422526,\n",
       "  0.005209741184059503,\n",
       "  0.005191319237268388,\n",
       "  0.0051729649350001,\n",
       "  0.00515463954283762,\n",
       "  0.005136778603069286,\n",
       "  0.005118029390891387,\n",
       "  0.005100030680034586,\n",
       "  0.00508222022149464,\n",
       "  0.005064625120657236,\n",
       "  0.005047033279541875,\n",
       "  0.005029753672993416,\n",
       "  0.005012302608156916,\n",
       "  0.004994836072879268,\n",
       "  0.004977673652530252,\n",
       "  0.004960749502005808,\n",
       "  0.004943622091047605,\n",
       "  0.004926911828267384,\n",
       "  0.004910050696608799,\n",
       "  0.0048935494256767825,\n",
       "  0.0048775248200043485,\n",
       "  0.004860619854298815,\n",
       "  0.004844521805852133,\n",
       "  0.00482821471359531,\n",
       "  0.004812078861721341,\n",
       "  0.004796367229374768,\n",
       "  0.004780285480204613,\n",
       "  0.004764657394221861,\n",
       "  0.004748698861828723,\n",
       "  0.004733023572844969,\n",
       "  0.004717438462403533,\n",
       "  0.004701917231559372,\n",
       "  0.004686598557015776,\n",
       "  0.004671362562971546,\n",
       "  0.004656256502160199,\n",
       "  0.00464110120291647,\n",
       "  0.004626099580905798,\n",
       "  0.004611230990994321,\n",
       "  0.004596349595393011,\n",
       "  0.004581657188642643,\n",
       "  0.004566954436437581,\n",
       "  0.004552445988727392,\n",
       "  0.00453794835743735,\n",
       "  0.004523494400378598,\n",
       "  0.00450917430708993,\n",
       "  0.0044950934773811705,\n",
       "  0.004480827288481321,\n",
       "  0.0044667992375869384,\n",
       "  0.00445267092812081,\n",
       "  0.004438799595853825,\n",
       "  0.00442484966541765,\n",
       "  0.004411057287288909,\n",
       "  0.004397290273941155,\n",
       "  0.004383705925752901,\n",
       "  0.004370056821088258,\n",
       "  0.004356707100331503,\n",
       "  0.004343207662604065,\n",
       "  0.0043297637085786085,\n",
       "  0.00431652388686047,\n",
       "  0.004303137313178655,\n",
       "  0.004289998013986255,\n",
       "  0.004276823442209902,\n",
       "  0.004263852420710589,\n",
       "  0.00425088380978556,\n",
       "  0.004237934784270178,\n",
       "  0.004225122392374868,\n",
       "  0.004212267864105172,\n",
       "  0.0041994561444102805,\n",
       "  0.0041867561690741035,\n",
       "  0.004174261998316169,\n",
       "  0.004161599844289446,\n",
       "  0.0041490440106191386],\n",
       " [])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True, monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e7148-a3ab-4f29-8473-5586eb8e8dfa",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369a217b-674b-4fca-8ebe-29c0709ab690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1175.3274228735127\n",
      "Accuracy on training data: 533 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1165.2704373173228\n",
      "Accuracy on training data: 766 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1156.0327073403653\n",
      "Accuracy on training data: 809 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1147.121681797924\n",
      "Accuracy on training data: 836 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1138.3169487488597\n",
      "Accuracy on training data: 880 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1129.5787235914045\n",
      "Accuracy on training data: 902 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1120.8951943517402\n",
      "Accuracy on training data: 911 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1112.3305228221554\n",
      "Accuracy on training data: 936 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1103.896112403029\n",
      "Accuracy on training data: 937 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1095.4946635983383\n",
      "Accuracy on training data: 947 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1087.1488418645818\n",
      "Accuracy on training data: 957 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1078.7370198550182\n",
      "Accuracy on training data: 965 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1070.4034410692072\n",
      "Accuracy on training data: 967 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1062.0559673586888\n",
      "Accuracy on training data: 978 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1053.7331355307942\n",
      "Accuracy on training data: 977 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1045.388319558065\n",
      "Accuracy on training data: 978 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1037.0849988973525\n",
      "Accuracy on training data: 981 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1028.7959926886258\n",
      "Accuracy on training data: 980 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1020.4945185675064\n",
      "Accuracy on training data: 984 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1012.2526640284867\n",
      "Accuracy on training data: 984 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1004.0384519122418\n",
      "Accuracy on training data: 987 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 995.8555401969564\n",
      "Accuracy on training data: 987 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 987.7287553114394\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 979.6495841347709\n",
      "Accuracy on training data: 988 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 971.6250551953224\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 963.6269411315332\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 955.6609202285745\n",
      "Accuracy on training data: 990 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 947.7453933523511\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 939.8885404409598\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 932.0381894349243\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 924.2282852059219\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 916.4999077091911\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 908.79335504141\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 901.1679872136563\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 893.5699423855377\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 886.0375260113377\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 878.5548633139356\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 871.1380670498792\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 863.7896499634594\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 856.4773523683291\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 849.2098746104103\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 842.0147470286842\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 834.8709107887673\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 827.7728882376773\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 820.7406339549008\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 813.7695567161686\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 806.8290939329204\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 799.9542198802072\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 793.1316224001525\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 786.3664418186166\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 779.6485022787944\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 772.9985698927212\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 766.394051742242\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 759.8530239773492\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 753.3580080011758\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 746.9250491067661\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 740.5517573920146\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 734.2310797736252\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 727.9745065878757\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 721.7467085585143\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 715.584827926547\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 709.4699544317289\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 703.401657569827\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 697.4042753636154\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 691.4384470258464\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 685.535871500848\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 679.6751327301575\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 673.8569287138996\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 668.1015988861586\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 662.3981743474372\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 656.7346960092541\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 651.1239633995223\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 645.5587599220895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 640.0511856153873\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 634.5866693161844\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 629.1728920658841\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 623.8051948495921\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 618.482698729482\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 613.2096477756076\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 607.9940651983011\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 602.8202030162365\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 597.6946235371798\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 592.6163083753378\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 587.5779517410325\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 582.5924521965838\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 577.6501142652965\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 572.756156078101\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 567.9056959498768\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 563.0965729031211\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 558.332569925295\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 553.6131999179189\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 548.9381167423153\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 544.3152992219124\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 539.7229825068766\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 535.181487275694\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 530.6767297950322\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 526.2169248212291\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 521.802825425233\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 517.421825853938\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 513.0863870327776\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 508.7882714762167\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 504.5314222777757\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 500.31628606043716\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 496.1360051694688\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 492.0007132965542\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 487.9004315806163\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 483.841728390793\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 479.8176032342716\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 475.83464223372783\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 471.89283882088984\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 467.9867036949557\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 464.1111718241322\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 460.2749553358059\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 456.4756235616038\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 452.70928506861026\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 448.98948865985847\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 445.28911284415574\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 441.63870397275514\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 438.0109979669048\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 434.4304734266643\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 430.87673682120607\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 427.36389443318484\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 423.88083337617934\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 420.43561740863703\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 417.0197854772069\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 413.6329737706141\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 410.28210629390617\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 406.97051509171746\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 403.68319956977626\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 400.4279263462941\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 397.21192521219814\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 394.0125891248633\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 390.8542064395565\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 387.72533681060196\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 384.62634185546165\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 381.5574582769819\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 378.5160133973472\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 375.5072645452478\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 372.52915154725264\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 369.577196319966\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 366.6550853137531\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 363.7679762207196\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 360.90285393555695\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 358.06689893199325\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 355.25895313153615\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 352.4756657472557\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 349.72736466802996\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 346.9995355169783\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 344.30103118182274\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 341.6208042474606\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 338.97760729624144\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 336.36075522966115\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 333.755681858738\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 331.1905453461597\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 328.64993276486916\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 326.13420109812154\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 323.6386294618815\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 321.16423116447936\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 318.7178140892852\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 316.2933783298078\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 313.9035351849777\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 311.52433420070037\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 309.1726241356399\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 306.8476059282607\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 304.5385594612381\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 302.2606355271963\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 299.99617466313924\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 297.7538028523507\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 295.5406964487777\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 293.3388943289841\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 291.17246026703293\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 289.0229660428527\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 286.89289684677783\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 284.77986761700697\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 282.6943708417195\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 280.62982373266067\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 278.58301891541703\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 276.5563432129647\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 274.5480837606148\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 272.56055798568735\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 270.5950171958602\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 268.64845139829134\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 266.71870291100157\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 264.8072703955311\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 262.92202691923336\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 261.051699701086\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 259.2010146237273\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 257.3672886591945\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 255.5457228731527\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 253.74731712783864\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 251.96677877828665\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 250.20611871956763\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 248.4587282659675\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 246.72598547477028\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 245.01509102344585\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 243.31986736150344\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 241.6421108500496\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 239.97863214993475\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 238.33442294323467\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 236.71105433545802\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 235.09758937314493\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 233.49683947860908\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 231.91854120875945\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 230.3560785281345\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 228.80956308692492\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 227.27237052553795\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 225.75411799865725\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 224.2432146677884\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 222.75503335926052\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 221.2781353296648\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 219.81904744499383\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 218.37504651419914\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 216.94300722418325\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 215.5283152673533\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 214.12301299463203\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 212.7306842367149\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 211.35704613721222\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 209.99746632837494\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 208.65283881872054\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 207.3193288357119\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 205.99524058358418\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 204.68664484741223\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 203.39021469159144\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 202.10408225273457\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 200.83381488915634\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 199.5772811611876\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 198.33283486103937\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 197.10105600746144\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 195.87881152322865\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 194.67214894597348\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 193.47201226462502\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 192.29019145762106\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 191.11238464133592\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 189.9531761311112\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 188.80437311752445\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 187.66734893569424\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 186.53961937668106\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 185.41894945693986\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 184.3160970956942\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 183.2187310450586\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 182.1367379285209\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 181.06706399480944\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 180.00033806407006\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 178.94754999436117\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 177.91156751460213\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 176.87912126646577\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 175.85868863358118\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 174.8466966988087\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 173.84619481335722\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 172.85440031450005\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 171.87212828126613\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 170.89868979838545\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 169.93612178436138\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 168.98661926169436\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 168.04094543611822\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 167.1125416265583\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 166.1853894629779\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 165.26979421178433\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 164.36536783098376\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 163.46890154922278\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 162.57835804270408\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 161.70601004414885\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 160.8273950753628\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 159.96472229423617\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 159.10618815084348\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 158.25952897670078\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 157.42686583913073\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 156.59995519342763\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 155.7702560056652\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 154.95796551644187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 154.1549877808971\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 153.3539567013798\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 152.56791798541968\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 151.77911909851537\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 151.00479723119457\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 150.23925393949904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 149.47760598616208\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 148.7313493071394\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 147.98396666547143\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 147.24255991368102\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 146.5129635259201\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 145.79253328970788\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 145.0731261696231\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 144.36632102028017\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 143.66321179350248\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 142.97115740567787\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 142.27983380205401\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 141.60223151952732\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 140.92761034714363\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 140.25493448136632\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 139.59255841581768\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 138.9368122312341\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 138.28772641704703\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 137.63919602045655\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 137.00402730076252\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 136.37417411016594\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 135.74372840639248\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 135.13331743218558\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 134.51427196017332\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 133.9048724991553\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 133.30320869718622\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 132.70938480285716\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 132.11716160448597\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 131.53494299056857\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 130.95410965511414\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 130.381031551637\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 129.81578486955448\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 129.25806550511922\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 128.69937332273886\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 128.14981666322947\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 127.60048752029051\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 127.06107207616955\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 126.53203797174599\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 125.99669240605019\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 125.47673936754676\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 124.9575107738231\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 124.44411701696272\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 123.93538631795279\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 123.43274004046644\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 122.93014237364174\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 122.43943845743634\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 121.95128479449701\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 121.46114476359381\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 120.98147218834524\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 120.50793888017888\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 120.03763299478204\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 119.56808688294755\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 119.10102643965213\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 118.64304551165553\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 118.19089463185924\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 117.73106559939046\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 117.28819769353784\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 116.84306022429489\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 116.40377995739772\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 115.97098781460019\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 115.54757899796581\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 115.1214884062294\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 114.69435955023688\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 114.28577166007193\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 113.87540953488077\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 113.46211449092354\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 113.05363063935465\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 112.65318283558284\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 112.25823345982802\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 111.86228441703125\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 111.47187364525053\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 111.08907617523987\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 110.70758459545152\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 110.3312547752532\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 109.9546929172404\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 109.58319684680264\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 109.21270145907192\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 108.84247825260601\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 108.48796153061113\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 108.12954944168347\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 107.77499606084632\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 107.43042918698544\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 107.07797971246869\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 106.73546347728197\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 106.39202785017785\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 106.05247392888737\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 105.72134061394176\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 105.38898527764427\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 105.05592674467978\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 104.73003544828683\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 104.40817559313307\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 104.08926442045296\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 103.77202382627547\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 103.45806244842612\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 103.14640204346962\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 102.8378861448595\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 102.53371663365758\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 102.22879606566109\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 101.93209348196031\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 101.6390146447892\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 101.3422702375909\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 101.05755646443497\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 100.77129947674187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 100.48444161971224\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 100.20232336018691\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 99.91958143749669\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 99.64660745773772\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 99.37052306785111\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 99.10076597131533\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 98.83277476636103\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 98.56835583694715\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 98.30449045959269\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 98.03635622638247\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 97.77864124000887\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 97.52569991874373\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 97.27533834046062\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 97.01951945688265\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 96.77134908607943\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 96.52865266695193\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 96.28265596038382\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 96.03819579303793\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 95.79936446063807\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 95.56498901992096\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 95.33020133064902\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 95.09503655014682\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.0\n",
      "Accuracy on evaluation data: 0 / 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1175.3274228735127,\n",
       "  1165.2704373173228,\n",
       "  1156.0327073403653,\n",
       "  1147.121681797924,\n",
       "  1138.3169487488597,\n",
       "  1129.5787235914045,\n",
       "  1120.8951943517402,\n",
       "  1112.3305228221554,\n",
       "  1103.896112403029,\n",
       "  1095.4946635983383,\n",
       "  1087.1488418645818,\n",
       "  1078.7370198550182,\n",
       "  1070.4034410692072,\n",
       "  1062.0559673586888,\n",
       "  1053.7331355307942,\n",
       "  1045.388319558065,\n",
       "  1037.0849988973525,\n",
       "  1028.7959926886258,\n",
       "  1020.4945185675064,\n",
       "  1012.2526640284867,\n",
       "  1004.0384519122418,\n",
       "  995.8555401969564,\n",
       "  987.7287553114394,\n",
       "  979.6495841347709,\n",
       "  971.6250551953224,\n",
       "  963.6269411315332,\n",
       "  955.6609202285745,\n",
       "  947.7453933523511,\n",
       "  939.8885404409598,\n",
       "  932.0381894349243,\n",
       "  924.2282852059219,\n",
       "  916.4999077091911,\n",
       "  908.79335504141,\n",
       "  901.1679872136563,\n",
       "  893.5699423855377,\n",
       "  886.0375260113377,\n",
       "  878.5548633139356,\n",
       "  871.1380670498792,\n",
       "  863.7896499634594,\n",
       "  856.4773523683291,\n",
       "  849.2098746104103,\n",
       "  842.0147470286842,\n",
       "  834.8709107887673,\n",
       "  827.7728882376773,\n",
       "  820.7406339549008,\n",
       "  813.7695567161686,\n",
       "  806.8290939329204,\n",
       "  799.9542198802072,\n",
       "  793.1316224001525,\n",
       "  786.3664418186166,\n",
       "  779.6485022787944,\n",
       "  772.9985698927212,\n",
       "  766.394051742242,\n",
       "  759.8530239773492,\n",
       "  753.3580080011758,\n",
       "  746.9250491067661,\n",
       "  740.5517573920146,\n",
       "  734.2310797736252,\n",
       "  727.9745065878757,\n",
       "  721.7467085585143,\n",
       "  715.584827926547,\n",
       "  709.4699544317289,\n",
       "  703.401657569827,\n",
       "  697.4042753636154,\n",
       "  691.4384470258464,\n",
       "  685.535871500848,\n",
       "  679.6751327301575,\n",
       "  673.8569287138996,\n",
       "  668.1015988861586,\n",
       "  662.3981743474372,\n",
       "  656.7346960092541,\n",
       "  651.1239633995223,\n",
       "  645.5587599220895,\n",
       "  640.0511856153873,\n",
       "  634.5866693161844,\n",
       "  629.1728920658841,\n",
       "  623.8051948495921,\n",
       "  618.482698729482,\n",
       "  613.2096477756076,\n",
       "  607.9940651983011,\n",
       "  602.8202030162365,\n",
       "  597.6946235371798,\n",
       "  592.6163083753378,\n",
       "  587.5779517410325,\n",
       "  582.5924521965838,\n",
       "  577.6501142652965,\n",
       "  572.756156078101,\n",
       "  567.9056959498768,\n",
       "  563.0965729031211,\n",
       "  558.332569925295,\n",
       "  553.6131999179189,\n",
       "  548.9381167423153,\n",
       "  544.3152992219124,\n",
       "  539.7229825068766,\n",
       "  535.181487275694,\n",
       "  530.6767297950322,\n",
       "  526.2169248212291,\n",
       "  521.802825425233,\n",
       "  517.421825853938,\n",
       "  513.0863870327776,\n",
       "  508.7882714762167,\n",
       "  504.5314222777757,\n",
       "  500.31628606043716,\n",
       "  496.1360051694688,\n",
       "  492.0007132965542,\n",
       "  487.9004315806163,\n",
       "  483.841728390793,\n",
       "  479.8176032342716,\n",
       "  475.83464223372783,\n",
       "  471.89283882088984,\n",
       "  467.9867036949557,\n",
       "  464.1111718241322,\n",
       "  460.2749553358059,\n",
       "  456.4756235616038,\n",
       "  452.70928506861026,\n",
       "  448.98948865985847,\n",
       "  445.28911284415574,\n",
       "  441.63870397275514,\n",
       "  438.0109979669048,\n",
       "  434.4304734266643,\n",
       "  430.87673682120607,\n",
       "  427.36389443318484,\n",
       "  423.88083337617934,\n",
       "  420.43561740863703,\n",
       "  417.0197854772069,\n",
       "  413.6329737706141,\n",
       "  410.28210629390617,\n",
       "  406.97051509171746,\n",
       "  403.68319956977626,\n",
       "  400.4279263462941,\n",
       "  397.21192521219814,\n",
       "  394.0125891248633,\n",
       "  390.8542064395565,\n",
       "  387.72533681060196,\n",
       "  384.62634185546165,\n",
       "  381.5574582769819,\n",
       "  378.5160133973472,\n",
       "  375.5072645452478,\n",
       "  372.52915154725264,\n",
       "  369.577196319966,\n",
       "  366.6550853137531,\n",
       "  363.7679762207196,\n",
       "  360.90285393555695,\n",
       "  358.06689893199325,\n",
       "  355.25895313153615,\n",
       "  352.4756657472557,\n",
       "  349.72736466802996,\n",
       "  346.9995355169783,\n",
       "  344.30103118182274,\n",
       "  341.6208042474606,\n",
       "  338.97760729624144,\n",
       "  336.36075522966115,\n",
       "  333.755681858738,\n",
       "  331.1905453461597,\n",
       "  328.64993276486916,\n",
       "  326.13420109812154,\n",
       "  323.6386294618815,\n",
       "  321.16423116447936,\n",
       "  318.7178140892852,\n",
       "  316.2933783298078,\n",
       "  313.9035351849777,\n",
       "  311.52433420070037,\n",
       "  309.1726241356399,\n",
       "  306.8476059282607,\n",
       "  304.5385594612381,\n",
       "  302.2606355271963,\n",
       "  299.99617466313924,\n",
       "  297.7538028523507,\n",
       "  295.5406964487777,\n",
       "  293.3388943289841,\n",
       "  291.17246026703293,\n",
       "  289.0229660428527,\n",
       "  286.89289684677783,\n",
       "  284.77986761700697,\n",
       "  282.6943708417195,\n",
       "  280.62982373266067,\n",
       "  278.58301891541703,\n",
       "  276.5563432129647,\n",
       "  274.5480837606148,\n",
       "  272.56055798568735,\n",
       "  270.5950171958602,\n",
       "  268.64845139829134,\n",
       "  266.71870291100157,\n",
       "  264.8072703955311,\n",
       "  262.92202691923336,\n",
       "  261.051699701086,\n",
       "  259.2010146237273,\n",
       "  257.3672886591945,\n",
       "  255.5457228731527,\n",
       "  253.74731712783864,\n",
       "  251.96677877828665,\n",
       "  250.20611871956763,\n",
       "  248.4587282659675,\n",
       "  246.72598547477028,\n",
       "  245.01509102344585,\n",
       "  243.31986736150344,\n",
       "  241.6421108500496,\n",
       "  239.97863214993475,\n",
       "  238.33442294323467,\n",
       "  236.71105433545802,\n",
       "  235.09758937314493,\n",
       "  233.49683947860908,\n",
       "  231.91854120875945,\n",
       "  230.3560785281345,\n",
       "  228.80956308692492,\n",
       "  227.27237052553795,\n",
       "  225.75411799865725,\n",
       "  224.2432146677884,\n",
       "  222.75503335926052,\n",
       "  221.2781353296648,\n",
       "  219.81904744499383,\n",
       "  218.37504651419914,\n",
       "  216.94300722418325,\n",
       "  215.5283152673533,\n",
       "  214.12301299463203,\n",
       "  212.7306842367149,\n",
       "  211.35704613721222,\n",
       "  209.99746632837494,\n",
       "  208.65283881872054,\n",
       "  207.3193288357119,\n",
       "  205.99524058358418,\n",
       "  204.68664484741223,\n",
       "  203.39021469159144,\n",
       "  202.10408225273457,\n",
       "  200.83381488915634,\n",
       "  199.5772811611876,\n",
       "  198.33283486103937,\n",
       "  197.10105600746144,\n",
       "  195.87881152322865,\n",
       "  194.67214894597348,\n",
       "  193.47201226462502,\n",
       "  192.29019145762106,\n",
       "  191.11238464133592,\n",
       "  189.9531761311112,\n",
       "  188.80437311752445,\n",
       "  187.66734893569424,\n",
       "  186.53961937668106,\n",
       "  185.41894945693986,\n",
       "  184.3160970956942,\n",
       "  183.2187310450586,\n",
       "  182.1367379285209,\n",
       "  181.06706399480944,\n",
       "  180.00033806407006,\n",
       "  178.94754999436117,\n",
       "  177.91156751460213,\n",
       "  176.87912126646577,\n",
       "  175.85868863358118,\n",
       "  174.8466966988087,\n",
       "  173.84619481335722,\n",
       "  172.85440031450005,\n",
       "  171.87212828126613,\n",
       "  170.89868979838545,\n",
       "  169.93612178436138,\n",
       "  168.98661926169436,\n",
       "  168.04094543611822,\n",
       "  167.1125416265583,\n",
       "  166.1853894629779,\n",
       "  165.26979421178433,\n",
       "  164.36536783098376,\n",
       "  163.46890154922278,\n",
       "  162.57835804270408,\n",
       "  161.70601004414885,\n",
       "  160.8273950753628,\n",
       "  159.96472229423617,\n",
       "  159.10618815084348,\n",
       "  158.25952897670078,\n",
       "  157.42686583913073,\n",
       "  156.59995519342763,\n",
       "  155.7702560056652,\n",
       "  154.95796551644187,\n",
       "  154.1549877808971,\n",
       "  153.3539567013798,\n",
       "  152.56791798541968,\n",
       "  151.77911909851537,\n",
       "  151.00479723119457,\n",
       "  150.23925393949904,\n",
       "  149.47760598616208,\n",
       "  148.7313493071394,\n",
       "  147.98396666547143,\n",
       "  147.24255991368102,\n",
       "  146.5129635259201,\n",
       "  145.79253328970788,\n",
       "  145.0731261696231,\n",
       "  144.36632102028017,\n",
       "  143.66321179350248,\n",
       "  142.97115740567787,\n",
       "  142.27983380205401,\n",
       "  141.60223151952732,\n",
       "  140.92761034714363,\n",
       "  140.25493448136632,\n",
       "  139.59255841581768,\n",
       "  138.9368122312341,\n",
       "  138.28772641704703,\n",
       "  137.63919602045655,\n",
       "  137.00402730076252,\n",
       "  136.37417411016594,\n",
       "  135.74372840639248,\n",
       "  135.13331743218558,\n",
       "  134.51427196017332,\n",
       "  133.9048724991553,\n",
       "  133.30320869718622,\n",
       "  132.70938480285716,\n",
       "  132.11716160448597,\n",
       "  131.53494299056857,\n",
       "  130.95410965511414,\n",
       "  130.381031551637,\n",
       "  129.81578486955448,\n",
       "  129.25806550511922,\n",
       "  128.69937332273886,\n",
       "  128.14981666322947,\n",
       "  127.60048752029051,\n",
       "  127.06107207616955,\n",
       "  126.53203797174599,\n",
       "  125.99669240605019,\n",
       "  125.47673936754676,\n",
       "  124.9575107738231,\n",
       "  124.44411701696272,\n",
       "  123.93538631795279,\n",
       "  123.43274004046644,\n",
       "  122.93014237364174,\n",
       "  122.43943845743634,\n",
       "  121.95128479449701,\n",
       "  121.46114476359381,\n",
       "  120.98147218834524,\n",
       "  120.50793888017888,\n",
       "  120.03763299478204,\n",
       "  119.56808688294755,\n",
       "  119.10102643965213,\n",
       "  118.64304551165553,\n",
       "  118.19089463185924,\n",
       "  117.73106559939046,\n",
       "  117.28819769353784,\n",
       "  116.84306022429489,\n",
       "  116.40377995739772,\n",
       "  115.97098781460019,\n",
       "  115.54757899796581,\n",
       "  115.1214884062294,\n",
       "  114.69435955023688,\n",
       "  114.28577166007193,\n",
       "  113.87540953488077,\n",
       "  113.46211449092354,\n",
       "  113.05363063935465,\n",
       "  112.65318283558284,\n",
       "  112.25823345982802,\n",
       "  111.86228441703125,\n",
       "  111.47187364525053,\n",
       "  111.08907617523987,\n",
       "  110.70758459545152,\n",
       "  110.3312547752532,\n",
       "  109.9546929172404,\n",
       "  109.58319684680264,\n",
       "  109.21270145907192,\n",
       "  108.84247825260601,\n",
       "  108.48796153061113,\n",
       "  108.12954944168347,\n",
       "  107.77499606084632,\n",
       "  107.43042918698544,\n",
       "  107.07797971246869,\n",
       "  106.73546347728197,\n",
       "  106.39202785017785,\n",
       "  106.05247392888737,\n",
       "  105.72134061394176,\n",
       "  105.38898527764427,\n",
       "  105.05592674467978,\n",
       "  104.73003544828683,\n",
       "  104.40817559313307,\n",
       "  104.08926442045296,\n",
       "  103.77202382627547,\n",
       "  103.45806244842612,\n",
       "  103.14640204346962,\n",
       "  102.8378861448595,\n",
       "  102.53371663365758,\n",
       "  102.22879606566109,\n",
       "  101.93209348196031,\n",
       "  101.6390146447892,\n",
       "  101.3422702375909,\n",
       "  101.05755646443497,\n",
       "  100.77129947674187,\n",
       "  100.48444161971224,\n",
       "  100.20232336018691,\n",
       "  99.91958143749669,\n",
       "  99.64660745773772,\n",
       "  99.37052306785111,\n",
       "  99.10076597131533,\n",
       "  98.83277476636103,\n",
       "  98.56835583694715,\n",
       "  98.30449045959269,\n",
       "  98.03635622638247,\n",
       "  97.77864124000887,\n",
       "  97.52569991874373,\n",
       "  97.27533834046062,\n",
       "  97.01951945688265,\n",
       "  96.77134908607943,\n",
       "  96.52865266695193,\n",
       "  96.28265596038382,\n",
       "  96.03819579303793,\n",
       "  95.79936446063807,\n",
       "  95.56498901992096,\n",
       "  95.33020133064902,\n",
       "  95.09503655014682],\n",
       " [533,\n",
       "  766,\n",
       "  809,\n",
       "  836,\n",
       "  880,\n",
       "  902,\n",
       "  911,\n",
       "  936,\n",
       "  937,\n",
       "  947,\n",
       "  957,\n",
       "  965,\n",
       "  967,\n",
       "  978,\n",
       "  977,\n",
       "  978,\n",
       "  981,\n",
       "  980,\n",
       "  984,\n",
       "  984,\n",
       "  987,\n",
       "  987,\n",
       "  991,\n",
       "  988,\n",
       "  989,\n",
       "  989,\n",
       "  990,\n",
       "  993,\n",
       "  993,\n",
       "  995,\n",
       "  994,\n",
       "  994,\n",
       "  995,\n",
       "  994,\n",
       "  994,\n",
       "  995,\n",
       "  997,\n",
       "  995,\n",
       "  997,\n",
       "  996,\n",
       "  997,\n",
       "  997,\n",
       "  998,\n",
       "  997,\n",
       "  997,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5, \n",
    "        evaluation_data=test_data, \n",
    "        lmbda=0.1, \n",
    "        monitor_evaluation_cost=True,\n",
    "        monitor_evaluation_accuracy=True, \n",
    "        monitor_training_cost=True,\n",
    "        monitor_training_accuracy=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea104395-4800-4aa7-85ae-60318ec2d761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumpyNeuralNetwork",
   "language": "python",
   "name": "numpyneuralnetwork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
